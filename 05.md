# CPU-scheduling

The images you uploaded contain Hungarian text related to operating system concepts, specifically focusing on resource management and scheduling.

Here’s a translation of the main content:

---

**5.1 Introduction**

The most important tasks of an OS:
- Managing system resources:
  - Efficient management
  - Protection
- Ensuring resources for processes:
  - Comfort
  - Security
  - Fairness

---

**Scheduling**

Definition:
Scheduling is the activity that determines which process will use a particular resource at the next moment.

---

Let me know if you need further details or assistance!

Here’s the translation of the new image:

---

**5.2 CPU Scheduling**

There are three different levels:
- Long-term (hosszútávú)
- Medium-term (középtávú)
- Short-term (rövidtávú)

Not all operating systems have all types of scheduling.

---

If you need more help or explanations, feel free to ask!

Here’s the English translation of the new content:

---

**Long-term Scheduling**

It decides which jobs from the background storage, which have not yet started, should begin running.
- Runs infrequently, so it doesn’t need to be fast.
- Requirements:
  - A job mix that uses system resources in a balanced manner.
  - Ensures both CPU-limited and peripheral-limited jobs are evenly distributed.

---

Let me know if you need further details or have more questions!





Here’s the English translation of the new content:

---

**Medium-term Scheduling**

The system tries to balance out periodic load fluctuations.

- **Suspension/activation of processes:**
  - In case of suspension, the process is stored in the background and is deprived of its resources.
  - Such processes no longer compete for resources.

---

Let me know if you'd like more details or have other questions!

Here’s the English translation of the content:

---

**Short-term Scheduling**

Which ready process gets the CPU.
- It runs frequently, so it must be fast; otherwise, the system would spend too much time scheduling, taking the CPU away from processes.
- The scheduler is always in the queue, as part of the OS kernel.

---

Feel free to ask if you need more explanations or have any further questions!

The image depicts a diagram of extended states for processes in a system, showing their transitions between different scheduling stages:

- **Long-term scheduling (Hosszútávú ütemezés)**: A process is created and moves into the "Ready to run" state.
- **Short-term scheduling (Rövidtávú ütemezés)**: A ready process gets the CPU to execute.
- **Medium-term scheduling (Középtávú ütemezés)**: Processes are suspended or activated based on system load.
- **Waiting (Várakozik)**: Processes that are waiting for an event are in this state.
- **Suspended (Felfüggesztve)**: Processes that have been suspended.
- **Event transitions**: The diagram shows how events cause changes in the states, like a process starting, suspending, or completing its execution.

If you need further details or explanations on the states or transitions, let me know!



Here’s the English translation of the content:

---

**5.3 Basics of Scheduling Algorithms**

- During the execution of a process, it performs two distinct types of activities:
  - **CPU burst**: The process needs only the CPU and central memory.
  - **I/O burst**: The process performs peripheral transfers, and the CPU is not required.
- The average duration of CPU and I/O bursts differs from one process to another.

---

This section highlights the two types of bursts (CPU and I/O) that a process undergoes during its execution and mentions how their lengths can vary between different processes.

Let me know if you need any further details!


In operating system scheduling, processes often go through two types of activities during their execution: **CPU bursts** and **I/O bursts**. Let me break down the concepts for you:

1. **CPU Burst**:
   - This is the period when a process is executing on the CPU. During a CPU burst, the process is performing computations or other operations that require the central processing unit (CPU).
   - **What it needs**: Only the CPU and central memory (RAM) are needed during this time. The CPU is actively processing the data and instructions of the process.

2. **I/O Burst**:
   - This is the period when the process is performing input or output operations, such as reading from or writing to disk, interacting with external devices, or transferring data between devices.
   - **What it needs**: The CPU is not involved during this phase. Instead, the process relies on peripheral devices such as disks, network devices, or other hardware for I/O operations.

### Scheduling Algorithms:
- **CPU Burst and I/O Burst** vary in duration from one process to another. Some processes might have longer CPU bursts (i.e., they spend more time calculating or processing data), while others might have longer I/O bursts (i.e., they spend more time waiting for data from peripherals).
- **Why this matters**: A good scheduling algorithm must take into account these two types of bursts to ensure efficient process management. The idea is to maximize CPU utilization while also allowing processes to perform I/O without unnecessarily blocking the CPU. For instance, if a process is waiting for I/O to complete, the CPU can be allocated to another process, improving overall system throughput.

### Example:
Imagine you have two processes:
- Process A is computationally heavy and spends most of its time performing calculations (CPU bursts).
- Process B, on the other hand, spends a lot of time waiting for data from an external device (I/O bursts).

In a well-designed scheduler, while Process A is working on its CPU burst, Process B can perform its I/O operations. When Process B is done with I/O, it can resume CPU work, and Process A can perform another I/O operation if needed. This allows the system to maximize its efficiency.

In summary:
- **CPU bursts** and **I/O bursts** are distinct phases of process execution.
- The duration of each burst can vary from process to process.
- Effective scheduling algorithms take these variations into account to optimize the use of CPU and I/O resources. 

Let me know if you want more details or have specific questions!







Here’s an explanation based on the content from the image you uploaded:

---

**CPU Bursts and I/O Bursts**

- **CPU bursts**: These are periods when a process is actively using the CPU. The process is performing computations or executing instructions.
  - In a **CPU-limited process**, the process mainly needs CPU and memory resources to continue executing, often with longer CPU bursts and shorter I/O operations.
  
- **I/O bursts**: These occur when a process waits for external devices or performs input/output operations. During this time, the process is not using the CPU but instead is involved in tasks like disk reads or writes.
  - In an **I/O-limited process**, the process spends most of its time waiting for I/O operations, with shorter CPU bursts.

---

**Explanation of the diagram**:
- The image shows two types of processes:
  - **CPU-limited processes** (represented by longer CPU bursts) are shown in diagram part (a).
  - **I/O-limited processes** (represented by shorter CPU bursts and longer waiting periods) are shown in diagram part (b).

This distinction is important for scheduling algorithms as processes with more I/O bursts typically don't need the CPU all the time, allowing other processes to run efficiently.

Let me know if you'd like further clarification!

This image continues from the previous one, illustrating the alternating **CPU bursts** and **I/O bursts** in a process execution.

In this diagram:
- **CPU Bursts**:
  - These are represented by actions like `load store`, `add store`, and `store increment`, which involve computations and operations that require the CPU.
- **I/O Bursts**:
  - These are represented by actions like `wait for I/O`, `read from file`, and `write to file`, where the process is waiting for I/O operations to complete, not using the CPU.

### Breakdown:
1. **First row**:
   - **CPU burst** (`load store`, `add store`)
   - **I/O burst** (`wait for I/O`, `read from file`)
2. **Second row**:
   - **CPU burst** (`store increment`, `index`)
   - **I/O burst** (`write to file`)
3. **Third row**:
   - **CPU burst** (`load store`, `add store`)
   - **I/O burst** (`wait for I/O`, `read from file`)

This shows how processes alternate between CPU and I/O bursts as part of their execution. The I/O burst doesn't need CPU resources, so while the process is waiting, other processes can use the CPU. Efficient scheduling algorithms consider these bursts to improve system throughput.

Let me know if you'd like more clarification!

This image illustrates the **distribution of CPU bursts** across a set of processes. The graph shows the relationship between the **CPU burst** time (on the horizontal axis) and the percentage of processes (on the vertical axis).

### Explanation:
- **X-axis (CPU burst)**: Represents the duration of CPU bursts.
- **Y-axis (Processes in %)**: Shows the percentage of processes that have a CPU burst of a specific duration.

The curve suggests that most processes have **shorter CPU bursts**, meaning they spend less time on the CPU before switching to an I/O burst. A few processes have **longer CPU bursts**, which occur less frequently.

This type of distribution is important for **scheduling algorithms**, as they must handle a variety of processes with different CPU burst durations efficiently.

Let me know if you need any further explanations!

This image explains when **scheduling** occurs in the context of process management in an operating system. The scheduling happens during the following state transitions:

1. **When a running process is forced to wait**: If a process cannot continue executing and must wait for some event or resource, the operating system will schedule another process to use the CPU.

2. **When a running process gives up the CPU or loses it**: If a process voluntarily releases the CPU or is preempted (such as in time-sharing systems), the operating system will schedule another process to run.

3. **When a process becomes ready to run (but there is no scheduling)**: The process may transition to the "ready" state, but scheduling may not always occur right away, depending on whether there is already a process using the CPU.

4. **When a running process finishes**: When a process completes its execution, the operating system will schedule another process to run in its place.

---

This process of scheduling ensures that the CPU is used efficiently and that processes are managed fairly. Let me know if you'd like to dive deeper into any of these transitions!



This diagram shows the process lifecycle states and transitions, specifically focusing on when a **running process has to wait** for an event.

### Key states and transitions:
- **Futásra kész (Ready to run)**: The process is ready to execute but is waiting for the CPU to become available.
- **Fut (Running)**: The process is currently executing on the CPU.
  - Transition to **Várakozik (Waiting)** happens when the running process has to wait for an event (e.g., I/O).
  - Transition from **Fut** to **Felfüggesztve kész (Suspended ready)** occurs when the process is suspended for some reason but still ready to resume.
- **Várakozik (Waiting)**: The process is waiting for some event to occur, such as I/O completion. This transition happens from **Fut** when the process cannot continue immediately.
- **Felfüggesztve kész (Suspended ready)**: The process is suspended but ready to be resumed.
- **Felfüggesztve várakozik (Suspended waiting)**: The process is suspended and waiting for an event. It cannot run or proceed until the event occurs.

### Active and Passive states:
- **Active states**: These are the states where the process is either executing or ready to execute, such as **Fut** and **Futásra kész**.
- **Passive states**: These are states where the process is suspended and not currently executing, such as **Várakozik** and **Felfüggesztve kész**.

The diagram represents a detailed view of the **active** and **passive** states, highlighting the flow of process states when waiting for an event or being suspended.

Let me know if you'd like further details!


This diagram focuses on the scenario where the **running process gives up the CPU** or **loses the CPU**. 

### Key states and transitions:
- **Fut (Running)**: The process is currently running and executing on the CPU.
  - The process may **give up the CPU** (for example, it may voluntarily release the CPU after a time slice in time-sharing systems) or the CPU may be taken away by the operating system (e.g., through preemption).
  - This results in the process moving from the **Fut** (Running) state to the **Futásra kész** (Ready to run) state.
- **Futásra kész (Ready to run)**: The process is ready to execute, but it is not currently using the CPU.
- **Várakozik (Waiting)**: The process transitions to this state when it is waiting for an event or I/O operation to complete. This happens when a process can’t proceed until the required event (like I/O) occurs.
- **Felfüggesztve kész (Suspended, ready)**: This state happens when the process is suspended but can resume when it is activated again. The process can be placed in this state for various reasons, such as prioritization or resource management.

### Active and Passive states:
- **Active states**: **Fut** and **Futásra kész** are considered active states since they either indicate execution or readiness for execution.
- **Passive states**: **Várakozik**, **Felfüggesztve kész**, and **Felfüggesztve várakozik** are passive states where the process isn't using the CPU.

### Transitions:
- A process in the **Fut (Running)** state may either be **preempted** (taken away by the OS) or voluntarily give up the CPU.
- The process then moves to the **Ready to run (Futásra kész)** state or **Waiting (Várakozik)** if it needs to wait for an event (like I/O).
- The **Felfüggesztve** (Suspended) states can also be reached from **Ready to run** or **Waiting** when the process is temporarily suspended.

This diagram helps visualize the dynamic process states during a CPU's preemption or voluntary release.

Let me know if you need further explanation!

This diagram focuses on the situation when a **running process becomes ready to run** (Futásra kész).

### Key states and transitions:
- **Fut (Running)**: The process is currently running on the CPU.
  - If the process is **interrupted** (for example, if it needs to wait for an event or a resource), it can transition to the **Várakozik (Waiting)** state.
  - After that, when the event (e.g., I/O) is completed or when the process is ready again, it moves to the **Futásra kész (Ready to run)** state, meaning it's ready for CPU allocation.
- **Futásra kész (Ready to run)**: After the process becomes ready, it waits in the ready queue for the CPU to become available.
- **Várakozik (Waiting)**: This state indicates the process is waiting for some event or resource. After waiting, it can transition to **Futásra kész** if the event occurs.
- **Felfüggesztve kész (Suspended ready)**: If the process is suspended, it can later be resumed and moved to the **Ready to run** state.

### Active and Passive states:
- **Active states**: The states **Fut** (Running) and **Futásra kész** (Ready to run) are active since the process is either executing or waiting to execute.
- **Passive states**: **Várakozik** (Waiting) and **Felfüggesztve kész** (Suspended ready) are passive states where the process is not using the CPU.

This diagram illustrates the lifecycle of processes, showing the transitions between different states as they become ready, run, or wait for events.

Let me know if you need more details!






In this diagram, the red area represents the state transition when a **running process completes** its execution. 

### Key transition:
- **Fut (Running)** → **Process terminates (megszűnik)**: This transition happens when the process finishes its task and is no longer needed to continue execution. The process is **terminated** and will no longer use system resources.

After the process finishes:
- It **exits** and the resources (like CPU and memory) it was using are released, allowing other processes to use them.

This final state marks the completion of the process lifecycle, where it no longer moves through the various active and passive states.

Let me know if you'd like further clarification or details!


The slide discusses the relationship between **scheduling** and **context switching** in the process lifecycle.

### **Scheduling** Steps:
1. **A running process is forced to wait**: The process cannot continue and must wait for an event (e.g., I/O operation).
2. **A running process gives up the CPU or loses it**: The process either voluntarily releases the CPU or is preempted by the operating system.
3. **A process becomes ready to run**: The process is now ready to execute again but must wait for the CPU to become available.
4. **A running process terminates**: The process completes its execution and terminates.

### **Context Switching**:
- **In steps 1 and 4**, **context switching** always happens. When a process moves from running to waiting (step 1) or when it terminates (step 4), the system saves the current state of the process and restores the state of the next process to execute.
  
- **In steps 2 and 3**, **context switching** does not always occur. A context switch happens when a process is preempted (step 2), but it may not occur if the CPU is simply reallocated from one process to another without switching the state entirely. Similarly, when a process becomes ready (step 3), it may not require a context switch depending on whether it needs to move to the CPU.

In summary:
- **Context switch** is guaranteed during **waiting** (step 1) and **termination** (step 4).
- It is **not always required** when a process either gives up the CPU (step 2) or becomes ready (step 3).

Let me know if you'd like further explanations!


The slide explains the two main types of **scheduling**: **Non-preemptive (cooperative)** and **Preemptive** scheduling.

### **Non-preemptive (Cooperative) Scheduling**:
- In this scheduling type, the **CPU cannot be taken away from a process** by the operating system.
- **A process can only change its state** based on its own instructions or actions. These actions include:
  1. Waiting for a resource or an event (like I/O).
  2. Termination of the process.
  3. The process voluntarily giving up the CPU (e.g., via a `yield` system call).

In this model, processes are cooperative in the sense that they manage their own state transitions. The OS cannot interrupt them at will, making it simpler but less flexible.

### **Preemptive Scheduling**:
- In **preemptive scheduling**, the **operating system can take the CPU away from a running process** at any time.
- When the OS takes over the CPU, the running process is moved to the **Ready to run (Futásra kész)** state, and another process in the ready queue is scheduled to run.

Preemptive scheduling provides better control over process execution and ensures more equitable distribution of CPU time, as the OS can prioritize processes based on various factors like time-sharing, priorities, or other scheduling algorithms.

Let me know if you'd like to explore these concepts further or need additional details!


This slide discusses the performance metrics used to evaluate **scheduling algorithms**:

### 1. **CPU Utilization**:
   - **Definition**: This measures the percentage of time the CPU is actively used for executing processes' instructions.
   - **Impact**: High CPU utilization indicates efficient usage, while low utilization means the CPU is idle (waiting for processes to be executed).
   - **Factors that reduce CPU utilization**: 
     - **CPU idle time**: When no processes are using the CPU.
     - **Overhead**: Time spent on system administration tasks that aren't related to process execution.

### 2. **Throughput**:
   - **Definition**: This metric indicates how many processes (jobs) the operating system can execute per unit of time.
   - **Importance**: Higher throughput means the OS is efficiently handling more tasks within a specific period, which is crucial for performance.

### 3. **Turnaround Time**:
   - **Definition**: This measures the total time taken for a process to complete, starting from the moment it enters the system until it finishes its execution.
   - **Impact**: The shorter the turnaround time, the faster the system can process and complete jobs. It includes waiting time, execution time, and I/O time.

These metrics help in comparing how different scheduling algorithms perform based on their efficiency in utilizing CPU resources, their ability to process tasks, and the time taken to complete processes.

Let me know if you'd like to dive deeper into any of these metrics!

This slide explains two additional important performance metrics used for evaluating scheduling algorithms:

### 1. **Waiting Time**:
   - **Definition**: Waiting time is the amount of time a process spends waiting for resources to become available, or while in different states like "Ready to run," "Waiting," "Suspended," or "Long-term waiting."
   - **Importance**: Minimizing waiting time ensures that processes spend less time idle, which improves overall system efficiency.

### 2. **Response Time**:
   - **Definition**: Response time is particularly important for **time-sharing** (interactive) systems. It measures the amount of time from when the user issues a command until the system's first visible response to that command.
   - **Importance**: In interactive systems, fast response time is crucial for user satisfaction. A shorter response time enhances the user experience by making the system feel more responsive.

These metrics focus on how well a system manages time and responsiveness, which are essential for both **batch processing systems** (where waiting time is critical) and **interactive systems** (where response time is a priority).

Let me know if you'd like more details or clarification!

This diagram shows various performance metrics in the form of a timeline, commonly used to evaluate scheduling algorithms. Let's break down the components:

### Key Areas:
- **Running (fut)**: Represents the time the process is actively running on the CPU. The process is performing its computation or operations.
- **Waiting (vár)**: Represents the time the process is waiting, likely due to I/O operations or waiting for resources.
- **Ready (kész)**: Represents the time the process is in the ready queue, waiting for the CPU to become available.

### Performance Metrics:
- **T_response**: This is the **response time**, which is the time taken for the system to respond to a user’s request, starting from when the user sends a command. The response time is measured from the user command to the first visible reaction from the system.
  
- **T_turnaround**: This is the **turnaround time**, which is the total time taken from when a process enters the system (its arrival) to when it finishes its execution.

- **T_waiting**: This is the **waiting time**, the amount of time a process spends waiting in the queue, either because it is not currently being executed or it is waiting for an event or I/O.

The diagram visually represents the flow of a process through different states and highlights where response, turnaround, and waiting times are measured in the lifecycle of a process.

Let me know if you need more details!














This slide outlines the requirements that scheduling algorithms should meet to be effective and efficient. Here's a breakdown of the listed requirements:

### 1. **Optimized according to some objective function**:
   - The scheduling algorithm should be designed to meet a specific optimization goal, which is typically based on the parameters defined by the system's performance criteria (e.g., maximizing CPU utilization or minimizing waiting time).

### 2. **Correctness and fairness**:
   - The algorithm must handle all processes (or specific types of processes) in the same manner, ensuring fairness. This means no process should be unfairly favored or neglected compared to others.

### 3. **Priority for certain processes**:
   - The algorithm should provide a way to prioritize certain processes when necessary. This ensures that more critical or urgent tasks receive the resources they need.

### 4. **Avoid starvation**:
   - The scheduling algorithm should prevent **starvation**, where some processes are perpetually denied CPU time or resources, ensuring that all processes eventually get executed.

### 5. **Predictable behavior**:
   - The algorithm should have predictable behavior, meaning it should be possible to estimate the maximum expected turnaround time for processes. This helps in ensuring that the system can meet performance expectations.

### 6. **Minimize overhead**:
   - The algorithm should minimize **overhead** (or "rezsi idő"), such as the time spent on administrative tasks, which can reduce the overall system performance. Lower overhead often leads to better system efficiency.

These requirements aim to make scheduling algorithms more efficient and predictable, while also ensuring that system resources are allocated fairly and optimally.

Let me know if you'd like more explanation or if you have further questions!


This slide outlines additional requirements for scheduling algorithms, focusing on resource management, system performance under increasing load, and setting clear system goals:

### 1. **Preferential Treatment**:
   - **Resources for underutilized processes**: The scheduling algorithm should prioritize processes that are underutilizing system resources, allowing them to make better use of the available resources.
   - **Important processes**: Processes that are utilizing critical resources should also be given priority to ensure that essential operations are completed.

### 2. **Graceful Degradation**:
   - **System performance under increasing load**: When the system is under heavy load, its performance should degrade **gradually and gracefully** (i.e., **graceful degradation**), meaning it should not suddenly crash or fail. This approach helps maintain some level of functionality even under stress.

### 3. **Conflicting Goals**:
   - **Conflicting objectives**: Many system goals might conflict with each other (e.g., maximizing throughput and minimizing response time). In such cases, it is impossible to fully satisfy all goals at once.

### 4. **Prioritizing Key Goals**:
   - **Setting key objectives**: During system design, it's important to clearly define which goals are of the highest priority. These goals should be focused on and optimized, even if trade-offs need to be made with less critical goals.

This slide emphasizes the need for an algorithm to handle resources efficiently, ensure system reliability under stress, and focus on optimizing the most important system objectives.

Let me know if you would like further details or clarification on any of these points!

This slide provides a detailed explanation of the **First Come First Served (FCFS)** scheduling algorithm.

### **FCFS (First Come First Served)**:
- **Concept**: The processes that are ready to execute are placed in the ready queue. The scheduler will start executing the process at the front of the queue (the one that arrived first).
- **Non-preemptive**: Once a process starts executing, it will continue to run until completion. The CPU is not taken away from it until it finishes.
- **Simple to implement**: This algorithm is straightforward and easy to implement because processes are simply executed in the order they arrive.
- **Convoy effect**: The downside is the **convoy effect**, where a long-running process (with a long CPU burst) can delay all the processes behind it in the queue. This causes inefficient use of the CPU and increases the waiting time for other processes.

### Key Points:
- **Advantages**:
  - Simple and easy to understand.
  - Ensures that the first process in the queue is always served first.
- **Disadvantages**:
  - The convoy effect: long processes can cause significant delays for shorter ones.
  - Can result in high average waiting time, especially when there are long processes at the start of the queue.

This slide is a good introduction to FCFS, but the next algorithms like **Shortest Job First (SJF)** and **Round-Robin (RR)** aim to address some of these inefficiencies.

Let me know if you'd like to continue exploring these or discuss more details!

This slide shows an example of the **First Come First Served (FCFS)** scheduling algorithm with calculations for average waiting time, average turnaround time, and throughput.

### Execution Order:
- **Order of Execution**: P1 → P2 → P3

### Calculations:
- **Average Waiting Time**: 
  $$
  \frac{(24 + 27)}{3} = 17 \text{ ms}
  $$
  This is the average time that all processes spend waiting in the ready queue before being executed.

- **Average Turnaround Time**: 
  $$
  \frac{(24 + 27 + 30)}{3} = 27 \text{ ms}
  $$
  This is the average time taken from when a process enters the system to when it completes its execution.

- **Throughput**:
  $$
  \frac{3 \text{ jobs}}{30 \text{ ms}} = \frac{1}{10} \text{ jobs/ms}
  $$
  This is the rate at which the system completes tasks.

### **Convoy Effect**:
- The **convoy effect** occurs because P1, a long process with a 24ms CPU burst, forces the other processes (P2 and P3) to wait, which leads to high waiting times for the shorter processes.

### Summary of FCFS:
- FCFS is simple and easy to implement, but it is inefficient when there are processes with significantly different execution times, as seen here with the **convoy effect**.
  
Let me know if you'd like further clarification or to explore more scheduling algorithms!

The slide discusses the **convoy effect**, which is a phenomenon that occurs in **cooperative scheduling** systems when a long CPU burst process blocks shorter processes, causing delays for the ready processes that follow.

### **Definition**:
- **Convoy Effect**: In a cooperative scheduling system, when a process with a long CPU burst (a long-running process) is executing, it prevents the shorter processes behind it in the queue from executing. This leads to the ready processes being delayed, waiting for the long-running process to finish.

### **Mitigation Strategies**:
- **Short Job First (SJF)**: Prioritize processes with short CPU bursts first, to avoid long processes holding up the queue.
- **Round-Robin (RR)**: Use a time-sharing system, where each process is given a time slice to execute. This prevents long processes from monopolizing the CPU and ensures fairer distribution of CPU time.

The convoy effect is a major issue in algorithms like **First Come First Served (FCFS)**, where long-running processes can cause significant delays for other processes in the queue.

Let me know if you'd like more details or need further clarification!

Here’s an implementation of the **First Come First Served (FCFS)** or **FIFO (First In, First Out)** CPU scheduling algorithm in C++:

This program calculates the **waiting time**, **turnaround time**, and **average waiting time** and **average turnaround time** for a set of processes.

```cpp
#include <iostream>
#include <vector>

using namespace std;

// Structure to represent a process
struct Process {
    int id; // Process ID
    int burst_time; // CPU burst time
    int arrival_time; // Arrival time
    int completion_time; // Completion time
    int waiting_time; // Waiting time
    int turnaround_time; // Turnaround time
};

// Function to calculate waiting time and turnaround time for each process
void calculateTimes(vector<Process>& processes) {
    int n = processes.size();
    int total_waiting_time = 0;
    int total_turnaround_time = 0;

    // The completion time for the first process is its burst time + arrival time
    processes[0].completion_time = processes[0].arrival_time + processes[0].burst_time;
    processes[0].turnaround_time = processes[0].completion_time - processes[0].arrival_time;
    processes[0].waiting_time = processes[0].turnaround_time - processes[0].burst_time;
    
    total_waiting_time += processes[0].waiting_time;
    total_turnaround_time += processes[0].turnaround_time;

    // For each subsequent process, calculate the completion, waiting, and turnaround times
    for (int i = 1; i < n; i++) {
        processes[i].completion_time = processes[i-1].completion_time + processes[i].burst_time;
        processes[i].turnaround_time = processes[i].completion_time - processes[i].arrival_time;
        processes[i].waiting_time = processes[i].turnaround_time - processes[i].burst_time;
        
        total_waiting_time += processes[i].waiting_time;
        total_turnaround_time += processes[i].turnaround_time;
    }

    // Calculate average waiting time and average turnaround time
    float average_waiting_time = (float)total_waiting_time / n;
    float average_turnaround_time = (float)total_turnaround_time / n;

    // Display the results
    cout << "Process ID\tBurst Time\tArrival Time\tCompletion Time\tWaiting Time\tTurnaround Time\n";
    for (int i = 0; i < n; i++) {
        cout << processes[i].id << "\t\t"
             << processes[i].burst_time << "\t\t"
             << processes[i].arrival_time << "\t\t"
             << processes[i].completion_time << "\t\t"
             << processes[i].waiting_time << "\t\t"
             << processes[i].turnaround_time << endl;
    }

    cout << "\nAverage Waiting Time: " << average_waiting_time << endl;
    cout << "Average Turnaround Time: " << average_turnaround_time << endl;
}

int main() {
    int n;
    cout << "Enter number of processes: ";
    cin >> n;
    
    vector<Process> processes(n);

    // Input process details
    for (int i = 0; i < n; i++) {
        processes[i].id = i + 1;
        cout << "Enter burst time and arrival time for process " << i + 1 << ": ";
        cin >> processes[i].burst_time >> processes[i].arrival_time;
    }

    // Sort processes by arrival time (important for FCFS)
    sort(processes.begin(), processes.end(), [](const Process& a, const Process& b) {
        return a.arrival_time < b.arrival_time;
    });

    // Calculate and display the times
    calculateTimes(processes);

    return 0;
}
```

### **Explanation of the Code**:
1. **Input**: The program first asks for the number of processes. Then, it takes the **burst time** and **arrival time** for each process.
2. **Sorting**: The processes are sorted based on their arrival time because FCFS schedules processes based on the order of their arrival.
3. **Calculations**:
   - **Completion Time**: The time when a process finishes execution.
   - **Turnaround Time**: The total time from the arrival of the process to its completion (`Completion Time - Arrival Time`).
   - **Waiting Time**: The total time the process spends waiting in the ready queue (`Turnaround Time - Burst Time`).
4. **Results**: The program calculates and displays the **average waiting time** and **average turnaround time**, and prints out a table with the details of each process.

### **Example**:

```
Enter number of processes: 3
Enter burst time and arrival time for process 1: 24 0
Enter burst time and arrival time for process 2: 3 1
Enter burst time and arrival time for process 3: 3 2

Process ID    Burst Time    Arrival Time    Completion Time    Waiting Time    Turnaround Time
1             24            0               24                0               24
2             3             1               27                23              26
3             3             2               30                25              28

Average Waiting Time: 16.5
Average Turnaround Time: 26
```

### **Key Points**:
- **FCFS** doesn’t prioritize processes based on their burst times, which can lead to inefficient scheduling, especially if there’s a long process that blocks shorter ones (known as the **convoy effect**).
- **Average Waiting Time** and **Turnaround Time** give an overall performance measure of the scheduling.

Let me know if you need more explanations or further assistance!


 
The slide explains the **Shortest Job First (SJF)** scheduling algorithm.

### **Shortest Job First (SJF)**:
- **Non-preemptive**: SJF is a non-preemptive algorithm, meaning once a process starts executing, it runs to completion without being interrupted. 
- **Choosing the Shortest Job**: Among the processes that are ready to execute, SJF selects the process with the shortest burst time (i.e., the shortest CPU execution time). The idea is that by executing the shortest tasks first, the overall waiting time and turnaround time are minimized.
- **No Convoy Effect**: Unlike **FCFS**, there is no convoy effect in SJF because shorter processes are prioritized over longer ones, preventing long tasks from delaying others.
- **Optimal Performance**: SJF results in the optimal **turnaround time** and **waiting time**, making it ideal in scenarios where the burst time of processes is known in advance.
  
### **Applications**:
- **Long-term Scheduling**: SJF can be used for long-term scheduling when processes are waiting to enter the system.
- **Short-term Scheduling**: It is also applicable in **real-time systems (RT)** where short tasks are prioritized to meet deadlines.

### **Pros**:
- **Optimal Performance**: SJF minimizes average waiting and turnaround times.

### **Cons**:
- **Starvation**: If a process has a long burst time and shorter processes keep coming, it may never get executed (i.e., it may starve).
- **Knowledge of Burst Time**: SJF requires prior knowledge of the burst times, which may not always be available in real-world scenarios.

This algorithm is particularly useful in environments where the job lengths are known and predictable, such as in batch processing systems.

Let me know if you need an example or C++ implementation of SJF!




This slide demonstrates the **Shortest Job First (SJF)** scheduling algorithm using an example. The table shows the **processes** and their **completion times (Ci)**.

### **Execution Order**:
- **Order of Execution**: P2 → P3 → P1

### **Calculations**:
- **Average Waiting Time**:
  $$
  \frac{(3 + 6)}{3} = 3 \text{ ms}
  $$
  This is the average time that processes spend in the ready queue waiting for execution.

- **Average Turnaround Time**:
  $$
  \frac{(3 + 6 + 30)}{3} = 13 \text{ ms}
  $$
  This is the average time taken from when a process arrives in the system to when it completes.

- **Throughput**:
  $$
  \frac{3 \text{ jobs}}{30 \text{ ms}} = \frac{1}{10} \text{ jobs/ms}
  $$
  This is the rate at which jobs are completed in the system.

### **Key Points**:
- **SJF** minimizes **average waiting time** and **average turnaround time**, making it an optimal algorithm for process scheduling when the burst times are known.
- There is no **convoy effect** in SJF, and it can efficiently handle processes of varying lengths.
- **Starvation** can occur if long processes keep getting delayed by shorter ones.

This example clearly shows how the shortest burst processes are given priority, which leads to better overall performance in terms of waiting and turnaround times.

If you need a deeper explanation or an implementation example, let me know!


Here's a C++ implementation of the **non-preemptive Shortest Job First (SJF)** scheduling algorithm. In this version, the processes are sorted by their burst time, and the scheduler selects the shortest job first to execute, without preempting the currently running process.

### C++ Code for **Non-preemptive SJF** CPU Scheduling:

```cpp
#include <iostream>
#include <vector>
#include <algorithm>

using namespace std;

// Structure to represent a process
struct Process {
    int id; // Process ID
    int burst_time; // CPU burst time
    int arrival_time; // Arrival time
    int completion_time; // Completion time
    int waiting_time; // Waiting time
    int turnaround_time; // Turnaround time
};

// Function to calculate waiting time and turnaround time for each process
void calculateTimes(vector<Process>& processes) {
    int n = processes.size();
    int total_waiting_time = 0;
    int total_turnaround_time = 0;

    // The completion time for the first process is its burst time + arrival time
    processes[0].completion_time = processes[0].arrival_time + processes[0].burst_time;
    processes[0].turnaround_time = processes[0].completion_time - processes[0].arrival_time;
    processes[0].waiting_time = processes[0].turnaround_time - processes[0].burst_time;
    
    total_waiting_time += processes[0].waiting_time;
    total_turnaround_time += processes[0].turnaround_time;

    // For each subsequent process, calculate the completion, waiting, and turnaround times
    for (int i = 1; i < n; i++) {
        processes[i].completion_time = processes[i-1].completion_time + processes[i].burst_time;
        processes[i].turnaround_time = processes[i].completion_time - processes[i].arrival_time;
        processes[i].waiting_time = processes[i].turnaround_time - processes[i].burst_time;
        
        total_waiting_time += processes[i].waiting_time;
        total_turnaround_time += processes[i].turnaround_time;
    }

    // Calculate average waiting time and average turnaround time
    float average_waiting_time = (float)total_waiting_time / n;
    float average_turnaround_time = (float)total_turnaround_time / n;

    // Display the results
    cout << "Process ID\tBurst Time\tArrival Time\tCompletion Time\tWaiting Time\tTurnaround Time\n";
    for (int i = 0; i < n; i++) {
        cout << processes[i].id << "\t\t"
             << processes[i].burst_time << "\t\t"
             << processes[i].arrival_time << "\t\t"
             << processes[i].completion_time << "\t\t"
             << processes[i].waiting_time << "\t\t"
             << processes[i].turnaround_time << endl;
    }

    cout << "\nAverage Waiting Time: " << average_waiting_time << endl;
    cout << "Average Turnaround Time: " << average_turnaround_time << endl;
}

// Function to sort processes by burst time (Shortest Job First)
bool compareBurstTime(Process& p1, Process& p2) {
    return p1.burst_time < p2.burst_time;
}

int main() {
    int n;
    cout << "Enter number of processes: ";
    cin >> n;
    
    vector<Process> processes(n);

    // Input process details
    for (int i = 0; i < n; i++) {
        processes[i].id = i + 1;
        cout << "Enter burst time and arrival time for process " << i + 1 << ": ";
        cin >> processes[i].burst_time >> processes[i].arrival_time;
    }

    // Sort processes by burst time (for SJF)
    sort(processes.begin(), processes.end(), compareBurstTime);

    // Calculate and display the times
    calculateTimes(processes);

    return 0;
}
```

### **Explanation**:

1. **Process Structure**: 
   - We define a `Process` structure to hold the `id`, `burst_time`, `arrival_time`, `completion_time`, `waiting_time`, and `turnaround_time` for each process.

2. **Input**:
   - The program asks for the number of processes and then takes input for the **burst time** and **arrival time** for each process.

3. **Sorting**:
   - The `processes` vector is sorted by the burst time using the `sort()` function and a comparison function `compareBurstTime()`. This ensures that the shortest job is always selected first for execution.

4. **Calculating Times**:
   - **Completion Time** is calculated for each process after sorting, based on the completion time of the previous process.
   - **Turnaround Time** is calculated as the difference between the **completion time** and **arrival time**.
   - **Waiting Time** is calculated as the difference between **turnaround time** and **burst time**.
   - Finally, the program calculates the **average waiting time** and **average turnaround time**.

5. **Output**:
   - The program displays a table of process information, including **waiting time** and **turnaround time**.
   - It also calculates and displays the **average waiting time** and **average turnaround time**.

### **Sample Output**:
```
Enter number of processes: 3
Enter burst time and arrival time for process 1: 24 0
Enter burst time and arrival time for process 2: 3 1
Enter burst time and arrival time for process 3: 3 2

Process ID    Burst Time    Arrival Time    Completion Time    Waiting Time    Turnaround Time
2             3             1               3                 0               2
3             3             2               6                 1               4
1             24            0               30                6               30

Average Waiting Time: 2.33333
Average Turnaround Time: 12
```

### **Key Points**:
- **SJF (Non-preemptive)**: Processes are executed in increasing order of burst time. Once a process starts, it runs to completion.
- **Efficiency**: SJF minimizes the **average waiting time** and **average turnaround time** for processes, making it optimal in terms of these metrics.
- **Starvation**: A potential downside of SJF is **starvation**, where longer processes may be indefinitely delayed if there are always shorter processes arriving.

Let me know if you need further explanations or modifications to the code!


The slide explains the **Round-Robin (RR)** CPU scheduling algorithm, which is **preemptive** and commonly used in **time-sharing systems**.

### Key Points of **Round-Robin (RR)** Scheduling:
1. **Preemptive**: The algorithm is preemptive, meaning that if a process does not finish its execution within a specified time quantum (also known as a **time slice**), it is interrupted and placed back in the ready queue to give other processes a chance to execute.
   
2. **Time Slice**: Each process is assigned a fixed time slice (quantum) during which it is allowed to execute.
   - **If the CPU burst time is greater than the time slice**, the process will complete its time slice, and the CPU is taken from it. The process will be moved to the back of the ready queue, and the next process will get a chance to execute.
   - **If the CPU burst time is shorter than the time slice**, the process will complete its execution within the allocated slice, and no preemption will happen. The next process is then scheduled.

3. **Context Switching**: If a process is preempted (because it uses more than its time slice), a **context switch** happens to save the state of the current process and load the state of the next process. The process will continue from where it left off in the next round.

### **Round-Robin Features**:
- **Fairness**: It ensures that each process gets an equal share of CPU time.
- **Time-sharing**: It works well in systems where many processes need to share the CPU in an equitable manner.
- **Performance**: While fair, Round-Robin might not be as efficient as other algorithms (like SJF or FCFS) for certain workloads, especially when processes have very different burst times.

### **Applications**:
- Round-Robin is widely used in **time-sharing systems** where multiple users or tasks need to share CPU time.
- It's also the basis for many scheduling algorithms used in **real-time systems** and **multitasking operating systems**.

Let me know if you'd like a C++ implementation for Round-Robin scheduling!









It seems you've uploaded another image related to the **Round-Robin (RR) scheduling**. Based on the content, this slide discusses the difficulty of determining an appropriate time slice (quantum) for Round-Robin scheduling.

### Key Points:
1. **Time Slice Determination**:
   - A **large time slice** would essentially behave like **FCFS (First Come First Served)** scheduling, meaning long processes could dominate the CPU.
   - A **small time slice** ensures equal CPU usage among processes but introduces **frequent context switching**, which reduces system performance.

2. **General Rule (Ökölszabály)**:
   - The rule suggests that the **CPU burst time** should be around **80%** of the time slice to minimize excessive context switching and make the system more efficient.

3. **Typical Time Slices**:
   - Time slices are usually set between **10-100 ms** in many systems.

Would you like to see an implementation for **Round-Robin Scheduling** in C++ or more details on how to optimize the time slice for this algorithm?

It looks like you uploaded an image containing an example for **Round-Robin (RR)** scheduling with a **time slice** of **4ms**.

### Breakdown:
1. **Execution Sequence**: The processes \( P1 \), \( P2 \), and \( P3 \) are scheduled in a round-robin manner with each receiving 4ms of CPU time (time slice).

2. **Metrics**:
   - **Average Waiting Time**: The average time a process spends in the ready queue before execution:
     
     $$
     \text{Average Waiting Time} = \frac{(6 + 4 + 7)}{3} = 5.66 \, \text{ms}
     $$
     
   - **Average Turnaround Time**: The average total time from the arrival of a process to its completion:
     
     $$
     \text{Average Turnaround Time} = \frac{(30 + 7 + 10)}{3} = 15.66 \, \text{ms}
     $$
     
   - **Throughput**: Number of jobs completed per unit of time:
     
     $$
     \text{Throughput} = \frac{3 \, \text{jobs}}{30 \, \text{ms}} = \frac{1}{10} \, \text{jobs/ms}
     $$

Would you like to see the C++ code for implementing this **Round-Robin scheduling** example?


Here's a basic C++ implementation of the **Round-Robin (RR)** scheduling algorithm with a fixed **time slice**:

### C++ Code:

```cpp
#include <iostream>
#include <queue>
#include <vector>

using namespace std;

// Structure to represent a process
struct Process {
    int id;             // Process ID
    int burst_time;     // CPU burst time
    int remaining_time; // Remaining burst time (for round-robin)
    int wait_time;      // Wait time in the ready queue
    int turn_around_time; // Turnaround time

    Process(int id, int burst_time) : id(id), burst_time(burst_time), remaining_time(burst_time), wait_time(0), turn_around_time(0) {}
};

// Function to calculate average waiting time and turnaround time
void calculateTimes(vector<Process>& processes) {
    int total_wait_time = 0;
    int total_turnaround_time = 0;
    
    for (auto& process : processes) {
        total_wait_time += process.wait_time;
        total_turnaround_time += process.turn_around_time;
    }
    
    int n = processes.size();
    cout << "Average Waiting Time: " << (float)total_wait_time / n << " ms" << endl;
    cout << "Average Turnaround Time: " << (float)total_turnaround_time / n << " ms" << endl;
}

// Round Robin scheduling algorithm
void roundRobin(vector<Process>& processes, int time_slice) {
    queue<Process*> q;
    int current_time = 0;
    
    // Push all processes into the queue
    for (auto& process : processes) {
        q.push(&process);
    }
    
    // Processing the queue
    while (!q.empty()) {
        Process* current_process = q.front();
        q.pop();
        
        int exec_time = min(current_process->remaining_time, time_slice);
        current_process->remaining_time -= exec_time;
        current_time += exec_time;

        // Update the wait time for other processes in the queue
        for (auto& process : processes) {
            if (&process != current_process && process.remaining_time > 0) {
                process.wait_time += exec_time;
            }
        }

        // If the process is completed
        if (current_process->remaining_time == 0) {
            current_process->turn_around_time = current_time;
            cout << "Process " << current_process->id << " completed at time " << current_time << " ms" << endl;
        }
        else {
            q.push(current_process); // Put it back in the queue if not finished
        }
    }

    // Calculate the average waiting and turnaround time
    calculateTimes(processes);
}

int main() {
    int n, time_slice;
    
    cout << "Enter the number of processes: ";
    cin >> n;
    
    vector<Process> processes;
    
    // Input processes
    for (int i = 0; i < n; ++i) {
        int burst_time;
        cout << "Enter burst time for Process " << i + 1 << ": ";
        cin >> burst_time;
        processes.push_back(Process(i + 1, burst_time));
    }
    
    // Time slice (quantum) for Round Robin
    cout << "Enter time slice (quantum) in ms: ";
    cin >> time_slice;
    
    // Start Round Robin scheduling
    roundRobin(processes, time_slice);
    
    return 0;
}
```

### Explanation:

1. **Process Structure**:
   - `id`: Process ID.
   - `burst_time`: Total burst time required by the process.
   - `remaining_time`: This keeps track of how much CPU time the process still needs.
   - `wait_time`: The amount of time a process spends waiting in the ready queue.
   - `turn_around_time`: The total time from the start to the completion of a process.

2. **Round-Robin Scheduling**:
   - The processes are enqueued and executed for a fixed **time slice** (quantum).
   - If the process completes its CPU burst within the time slice, its remaining time becomes zero, and the turnaround time is recorded.
   - If the process doesn’t finish in the time slice, it is placed back into the queue for the next round.
   - The **waiting time** is incremented for all processes in the queue after each round.

3. **Input/Output**:
   - The program takes the number of processes and the burst times of each process.
   - The time slice is also taken as input.
   - Finally, it prints the average **waiting time** and **turnaround time**.

### Example Run:
```
Enter the number of processes: 3
Enter burst time for Process 1: 24
Enter burst time for Process 2: 3
Enter burst time for Process 3: 3
Enter time slice (quantum) in ms: 4
Process 1 completed at time 24 ms
Process 2 completed at time 7 ms
Process 3 completed at time 10 ms
Average Waiting Time: 5.66667 ms
Average Turnaround Time: 15.6667 ms
```

This code will simulate the Round-Robin scheduling algorithm, showing the completion times of the processes, and the averages for waiting and turnaround times.

Here is a simple C++ implementation of the **Round-Robin Scheduling Algorithm** with a time slice:

### C++ Code for Round-Robin Scheduling:

```cpp
#include <iostream>
#include <queue>
#include <vector>
#include <algorithm>

using namespace std;

// Structure for Process
struct Process {
    int id;          // Process ID
    int burstTime;   // Burst Time (CPU time needed)
    int remainingTime; // Remaining Burst Time
    int arrivalTime; // Arrival time of the process (not used in basic RR)
    int completionTime; // Completion Time of the process
    int waitingTime;    // Waiting time for the process
    int turnaroundTime; // Turnaround time for the process
};

void roundRobinScheduling(vector<Process>& processes, int timeSlice) {
    int currentTime = 0;
    queue<Process*> readyQueue;

    // Initialize ready queue
    for (auto& p : processes) {
        p.remainingTime = p.burstTime;
        readyQueue.push(&p);
    }

    while (!readyQueue.empty()) {
        Process* p = readyQueue.front();
        readyQueue.pop();

        // If the remaining time of the process is greater than time slice, it will go back to ready queue after executing
        if (p->remainingTime > timeSlice) {
            p->remainingTime -= timeSlice;
            currentTime += timeSlice;
            readyQueue.push(p);
        } else {
            // Process completes in this time slice
            currentTime += p->remainingTime;
            p->completionTime = currentTime;
            p->turnaroundTime = p->completionTime;
            p->waitingTime = p->turnaroundTime - p->burstTime;
            p->remainingTime = 0; // Process has finished
        }
    }
}

// Function to display the results
void displayResults(const vector<Process>& processes) {
    int totalWaitingTime = 0, totalTurnaroundTime = 0;

    cout << "Process ID | Burst Time | Waiting Time | Turnaround Time | Completion Time" << endl;
    for (const auto& p : processes) {
        cout << "P" << p.id << "         | " << p.burstTime << "         | " << p.waitingTime 
             << "            | " << p.turnaroundTime << "            | " << p.completionTime << endl;
        totalWaitingTime += p.waitingTime;
        totalTurnaroundTime += p.turnaroundTime;
    }

    cout << "Average Waiting Time: " << (float)totalWaitingTime / processes.size() << endl;
    cout << "Average Turnaround Time: " << (float)totalTurnaroundTime / processes.size() << endl;
}

int main() {
    int n, timeSlice;

    // Taking input for number of processes
    cout << "Enter number of processes: ";
    cin >> n;
    
    vector<Process> processes(n);

    // Input process details
    for (int i = 0; i < n; ++i) {
        processes[i].id = i + 1;
        cout << "Enter Burst Time for Process " << processes[i].id << ": ";
        cin >> processes[i].burstTime;
    }

    // Enter the time slice (quantum)
    cout << "Enter time slice (in ms): ";
    cin >> timeSlice;

    // Perform Round Robin Scheduling
    roundRobinScheduling(processes, timeSlice);

    // Display the results
    displayResults(processes);

    return 0;
}
```

### Explanation:

1. **Process Structure**:
   - Each process has attributes like **id**, **burst time**, **remaining time** (for tracking the time slice), **completion time**, **waiting time**, and **turnaround time**.

2. **Round-Robin Scheduling**:
   - We maintain a queue of processes that are ready to be executed.
   - Each process gets the CPU for a time slice (quantum).
   - If a process still needs more time after its slice, it is put back in the queue.
   - If the process finishes its burst time within the time slice, we compute its completion time, turnaround time, and waiting time.

3. **Displaying Results**:
   - The results include **waiting time**, **turnaround time**, and **completion time** for each process.
   - Average waiting and turnaround times are computed at the end.

### Sample Input/Output:

#### Input:
```
Enter number of processes: 3
Enter Burst Time for Process 1: 24
Enter Burst Time for Process 2: 3
Enter Burst Time for Process 3: 3
Enter time slice (in ms): 4
```

#### Output:
```
Process ID | Burst Time | Waiting Time | Turnaround Time | Completion Time
P1         | 24         | 6            | 30              | 30
P2         | 3          | 4            | 7               | 7
P3         | 3          | 7            | 10              | 10
Average Waiting Time: 5.66667
Average Turnaround Time: 15.6667
```

This implementation will execute the **Round-Robin (RR)** algorithm, handle multiple processes, and compute the average waiting time and turnaround time. Let me know if you need further modifications or explanations!




The image discusses the impact of the time slice (Tslice) on the frequency of context switches in a Round Robin scheduling algorithm. The key points are:

- **Az időszelet hatása a környezetváltás gyakoriságára**: This translates to "The effect of the time slice (Tslice) on the frequency of context switches."
- **Ci = 10ms**: This indicates that the context switching time is set at 10ms.
- The table shows how different time slices (Tslice) affect the number of context switches:
  - With a **12ms** time slice, there are **0 context switches**.
  - With a **6ms** time slice, there is **1 context switch**.
  - With a **1ms** time slice, there are **9 context switches**.

This shows that a smaller time slice leads to more frequent context switches, while a larger time slice reduces the frequency. The overall behavior illustrates the trade-off between the time slice and the efficiency of the Round Robin scheduling algorithm.